name: Destroy EKS Cluster

on:
  workflow_dispatch:
    inputs:
      confirmation:
        description: 'Type "DESTROY" to confirm cluster destruction'
        required: true
        type: string
      cluster_name:
        description: 'EKS Cluster Name to destroy'
        required: true
        default: 'ticket-118-github-action-eks-cluster'
        type: string

env:
  TF_VERSION: '1.5.0'
  AWS_REGION: 'us-east-1'

jobs:
  destroy:
    name: 'Terraform Destroy'
    runs-on: ubuntu-latest
    
    # Use OIDC to assume role in AWS
    permissions:
      id-token: write
      contents: read

    steps:
    - name: Validate Destruction Confirmation
      run: |
        if [ "${{ github.event.inputs.confirmation }}" != "DESTROY" ]; then
          echo "‚ùå Destruction confirmation failed. You must type 'DESTROY' exactly to proceed."
          echo "You entered: '${{ github.event.inputs.confirmation }}'"
          exit 1
        fi
        echo "‚úÖ Destruction confirmed. Proceeding with cluster destruction."

    - name: Checkout Repository
      uses: actions/checkout@v4

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::855978188999:role/ticket-118-github-action-eks-cluster-github-actions-role
        role-session-name: GitHubActions-EKS-Destroy
        aws-region: ${{ env.AWS_REGION }}

    - name: Verify AWS Identity
      run: |
        echo "AWS Identity:"
        aws sts get-caller-identity
        echo "AWS Region: $AWS_REGION"

    - name: Verify Cluster Exists
      run: |
        echo "Checking if cluster exists..."
        if aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --region ${{ env.AWS_REGION }} &> /dev/null; then
          echo "‚úÖ Cluster '${{ github.event.inputs.cluster_name }}' found."
          aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --region ${{ env.AWS_REGION }} --query 'cluster.{Name:name,Status:status,Version:version,Endpoint:endpoint}' --output table
        else
          echo "‚ö†Ô∏è  Cluster '${{ github.event.inputs.cluster_name }}' not found or not accessible."
          echo "This might be expected if the cluster was already destroyed."
        fi

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Terraform Init
      run: |
        terraform init
        echo "Terraform initialized successfully"

    - name: Import Existing OIDC Provider (if needed)
      run: |
        echo "Checking if OIDC provider needs to be imported..."
        if ! terraform state list | grep -q "aws_iam_openid_connect_provider.github_actions"; then
          echo "Importing existing OIDC provider..."
          terraform import aws_iam_openid_connect_provider.github_actions arn:aws:iam::855978188999:oidc-provider/token.actions.githubusercontent.com || true
        else
          echo "OIDC provider already in state"
        fi

    - name: Import Existing Resources Before Destroy
      run: |
        echo "Importing all existing resources into Terraform state..."
        
        # Import OIDC provider
        terraform import aws_iam_openid_connect_provider.github_actions arn:aws:iam::855978188999:oidc-provider/token.actions.githubusercontent.com || echo "OIDC provider import failed"
        
        # Import GitHub Actions IAM role and policies
        terraform import aws_iam_role.github_actions_role ticket-118-github-action-eks-cluster-github-actions-role || echo "GitHub Actions role import failed"
        terraform import aws_iam_policy.github_actions_eks_policy arn:aws:iam::855978188999:policy/ticket-118-github-action-eks-cluster-github-actions-eks-policy || echo "EKS policy import failed"
        terraform import aws_iam_policy.github_actions_iam_policy arn:aws:iam::855978188999:policy/ticket-118-github-action-eks-cluster-github-actions-iam-policy || echo "IAM policy import failed"
        
        # Import EKS cluster IAM roles
        terraform import aws_iam_role.eks_cluster_role ticket-118-github-action-eks-cluster-cluster-role || echo "EKS cluster role import failed"
        terraform import aws_iam_role.eks_node_group_role ticket-118-github-action-eks-cluster-node-group-role || echo "EKS node role import failed"
        
        # Import security groups
        terraform import aws_security_group.eks_cluster_sg sg-044e60059716fe76b || echo "Cluster SG import failed"
        terraform import aws_security_group.eks_nodes_sg sg-08c19713020e246a2 || echo "Nodes SG import failed"
        
        # Import EKS cluster and node group
        terraform import aws_eks_cluster.main ticket-118-github-action-eks-cluster || echo "EKS cluster import failed"
        terraform import aws_eks_node_group.main ticket-118-github-action-eks-cluster:ticket-118-github-action-eks-cluster-nodes || echo "Node group import failed"
        
        # Import EKS addons
        terraform import aws_eks_addon.vpc_cni ticket-118-github-action-eks-cluster:vpc-cni || echo "VPC CNI addon import failed"
        terraform import aws_eks_addon.coredns ticket-118-github-action-eks-cluster:coredns || echo "CoreDNS addon import failed"
        terraform import aws_eks_addon.kube_proxy ticket-118-github-action-eks-cluster:kube-proxy || echo "Kube-proxy addon import failed"
        
        # Import policy attachments (these might fail but that's okay)
        terraform import aws_iam_role_policy_attachment.eks_cluster_policy ticket-118-github-action-eks-cluster-cluster-role/arn:aws:iam::aws:policy/AmazonEKSClusterPolicy || echo "Cluster policy attachment import failed"
        terraform import aws_iam_role_policy_attachment.eks_worker_node_policy ticket-118-github-action-eks-cluster-node-group-role/arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy || echo "Worker policy attachment import failed"
        terraform import aws_iam_role_policy_attachment.eks_cni_policy ticket-118-github-action-eks-cluster-node-group-role/arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy || echo "CNI policy attachment import failed"
        terraform import aws_iam_role_policy_attachment.eks_container_registry_policy ticket-118-github-action-eks-cluster-node-group-role/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly || echo "Registry policy attachment import failed"
        
        echo "Import phase completed. Some imports may have failed - this is expected for policy attachments."

    - name: Verify Imported State
      run: |
        echo "Checking what resources are now in Terraform state..."
        terraform state list
        echo "State verification completed."

    - name: Terraform Plan Destroy
      id: plan-destroy
      run: |
        echo "Planning destruction of EKS cluster..."
        terraform plan -destroy -out=destroy-plan -no-color
        echo "Destroy plan created successfully"

    - name: Show Destroy Plan Summary
      run: |
        echo "=== RESOURCES TO BE DESTROYED ==="
        terraform show -no-color destroy-plan | grep -E "(will be destroyed|Plan:|Changes to)"

    - name: Final Confirmation
      run: |
        echo "‚ö†Ô∏è  WARNING: This will permanently destroy the following:"
        echo "- EKS Cluster: ${{ github.event.inputs.cluster_name }}"
        echo "- Worker Node Groups"
        echo "- Security Groups"
        echo "- IAM Roles and Policies"
        echo "- CloudWatch Log Groups"
        echo ""
        echo "‚úÖ Confirmed by user: ${{ github.actor }}"
        echo "‚úÖ Confirmation text: ${{ github.event.inputs.confirmation }}"

    - name: Terraform Destroy
      run: |
        echo "üö® Starting cluster destruction..."
        terraform apply -auto-approve destroy-plan
        echo "‚úÖ Cluster destruction completed"

    - name: Verify Cluster Destruction
      run: |
        echo "Verifying cluster destruction..."
        sleep 30
        
        if aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --region ${{ env.AWS_REGION }} &> /dev/null; then
          echo "‚ö†Ô∏è  Cluster still exists - destruction may be in progress"
          aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text
        else
          echo "‚úÖ Cluster successfully destroyed"
        fi

    - name: Cleanup Local State (Optional)
      if: always()
      run: |
        echo "Cleaning up local Terraform state files..."
        ls -la terraform.tfstate* || echo "No state files to clean"

    - name: Destruction Summary
      if: always()
      run: |
        echo "=== DESTRUCTION SUMMARY ==="
        echo "Cluster Name: ${{ github.event.inputs.cluster_name }}"
        echo "Requested by: ${{ github.actor }}"
        echo "Timestamp: $(date)"
        echo "Status: ${{ job.status }}"